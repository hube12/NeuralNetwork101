{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"neural network.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PpJmvm12dBfL","colab_type":"text"},"source":["![alt text](https://miro.medium.com/max/582/1*dFbZFuwqoR2YIHS4YcC9Jg.jpeg)\n","\n","**A1 = h(W1*X + b1)**\n","\n","**A2 = g(W2*A1 + b2)**"]},{"cell_type":"markdown","metadata":{"id":"55i5mbmUeHBB","colab_type":"text"},"source":["Initialization"]},{"cell_type":"code","metadata":{"id":"N7nVZlqHdwBZ","colab_type":"code","colab":{}},"source":["\"\"\"\n","    Simple Neural Network with 1 hidden layer with the number\n","    of hidden units as a hyperparameter to calculate the XOR function\n","\"\"\"\n","\n","import numpy as np\n","\n","def initialize_parameters(n_x, n_h, n_y):\n","    W1 = np.random.randn(n_h, n_x)\n","    b1 = np.zeros((n_h, 1))\n","    W2 = np.random.randn(n_y, n_h)\n","    b2 = np.zeros((n_y, 1))\n","\n","    parameters = {\n","        \"W1\": W1,\n","        \"b1\" : b1,\n","        \"W2\": W2,\n","        \"b2\" : b2\n","    }\n","    return parameters"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"65lOjuiAd72D","colab_type":"text"},"source":["Activation function"]},{"cell_type":"code","metadata":{"id":"QlmtmJJQd9vp","colab_type":"code","colab":{}},"source":["def sigmoid(z):\n","    return 1/(1 + np.exp(-z))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"77T_70EXd-xU","colab_type":"text"},"source":["Forward propagation"]},{"cell_type":"code","metadata":{"id":"5dWi6RkRd-WB","colab_type":"code","colab":{}},"source":["def forward_prop(X, parameters):\n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","\n","    Z1 = np.dot(W1, X) + b1\n","    A1 = np.tanh(Z1) # first activation function\n","    Z2 = np.dot(W2, A1) + b2 \n","    A2 = sigmoid(Z2) # second activation function\n","\n","    cache = {\n","        \"A1\": A1,\n","        \"A2\": A2\n","    }\n","    return A2, cache"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qN1ThVdaeO-V","colab_type":"text"},"source":["Cost calculation\n"]},{"cell_type":"code","metadata":{"id":"nCPvXtJ2eSEL","colab_type":"code","colab":{}},"source":["def calculate_cost(A2, Y):\n","    cost = -np.sum(np.multiply(Y, np.log(A2)) +  np.multiply(1-Y, np.log(1-A2)))/m\n","    cost = np.squeeze(cost)\n","    return cost"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"07hNMr9VeTid","colab_type":"text"},"source":["Backward propagation\n"]},{"cell_type":"code","metadata":{"id":"F06akzt3eZOi","colab_type":"code","colab":{}},"source":["def backward_prop(X, Y, cache, parameters):\n","    A1 = cache[\"A1\"]\n","    A2 = cache[\"A2\"]\n","\n","    W2 = parameters[\"W2\"]\n","\n","    dZ2 = A2 - Y\n","    dW2 = np.dot(dZ2, A1.T)/m\n","    db2 = np.sum(dZ2, axis=1, keepdims=True)/m\n","    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1-np.power(A1, 2))\n","    dW1 = np.dot(dZ1, X.T)/m\n","    db1 = np.sum(dZ1, axis=1, keepdims=True)/m\n","\n","    grads = {\n","        \"dW1\": dW1,\n","        \"db1\": db1,\n","        \"dW2\": dW2,\n","        \"db2\": db2\n","    }\n","\n","    return grads"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yLOuJsSjebUx","colab_type":"text"},"source":["Update method"]},{"cell_type":"code","metadata":{"id":"ePpq5cMaeahJ","colab_type":"code","colab":{}},"source":["def update_parameters(parameters, grads, learning_rate):\n","    W1 = parameters[\"W1\"]\n","    b1 = parameters[\"b1\"]\n","    W2 = parameters[\"W2\"]\n","    b2 = parameters[\"b2\"]\n","\n","    dW1 = grads[\"dW1\"]\n","    db1 = grads[\"db1\"]\n","    dW2 = grads[\"dW2\"]\n","    db2 = grads[\"db2\"]\n","\n","    W1 = W1 - learning_rate*dW1\n","    b1 = b1 - learning_rate*db1\n","    W2 = W2 - learning_rate*dW2\n","    b2 = b2 - learning_rate*db2\n","    \n","    new_parameters = {\n","        \"W1\": W1,\n","        \"W2\": W2,\n","        \"b1\" : b1,\n","        \"b2\" : b2\n","    }\n","\n","    return new_parameters"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lh-xwtCDeeSg","colab_type":"text"},"source":["Building the network"]},{"cell_type":"code","metadata":{"id":"bnGuftPFehEv","colab_type":"code","colab":{}},"source":["def model(X, Y, n_x, n_h, n_y, num_of_iters, learning_rate):\n","    parameters = initialize_parameters(n_x, n_h, n_y)\n","\n","    for i in range(0, num_of_iters+1):\n","        a2, cache = forward_prop(X, parameters)\n","\n","        cost = calculate_cost(a2, Y)\n","\n","        grads = backward_prop(X, Y, cache, parameters)\n","\n","        parameters = update_parameters(parameters, grads, learning_rate)\n","\n","        if(i%100 == 0):\n","            print('Cost after iteration# {:d}: {:f}'.format(i, cost))\n","\n","    return parameters"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5dZrJWKxeqxG","colab_type":"text"},"source":["Prediction function"]},{"cell_type":"code","metadata":{"id":"f_wb3beOepei","colab_type":"code","colab":{}},"source":["def predict(X, parameters):\n","    a2, cache = forward_prop(X, parameters)\n","    yhat = a2\n","    yhat = np.squeeze(yhat)\n","    if(yhat >= 0.5):\n","        y_predict = 1\n","    else:\n","        y_predict = 0\n","\n","    return y_predict"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vrUNm0gDevrM","colab_type":"text"},"source":["Running the model"]},{"cell_type":"code","metadata":{"id":"_D8oiZSzexrD","colab_type":"code","outputId":"1b534799-51ff-4a1f-ed35-a51f782ba781","executionInfo":{"status":"ok","timestamp":1569881726171,"user_tz":-120,"elapsed":1485,"user":{"displayName":"neil","photoUrl":"","userId":"09670409907443135887"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# For Reproducibility\n","np.random.seed(2)\n","\n","# The 4 training examples by columns\n","X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n","\n","# The outputs of the XOR for every example in X\n","Y = np.array([[0, 1, 1, 0]])\n","\n","# No. of training examples\n","m = X.shape[1]\n","\n","# Set the hyperparameters\n","n_x = 2     #No. of neurons in first layer\n","n_h = 2     #No. of neurons in hidden layer\n","n_y = 1     #No. of neurons in output layer\n","num_of_iters = 10000\n","learning_rate = 0.3\n","\n","trained_parameters = model(X, Y, n_x, n_h, n_y, num_of_iters, learning_rate)\n","\n","# Test 2X1 vector to calculate the XOR of its elements. \n","# Try (0, 0), (0, 1), (1, 0), (1, 1)\n","X_test = np.array([[1], [1]])\n","\n","y_predict = predict(X_test, trained_parameters)\n","\n","print('Neural Network prediction for example ({:d}, {:d}) is {:d}'.format(\n","    X_test[0][0], X_test[1][0], y_predict))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cost after iteration# 0: 0.856267\n","Cost after iteration# 100: 0.347426\n","Cost after iteration# 200: 0.101195\n","Cost after iteration# 300: 0.053631\n","Cost after iteration# 400: 0.036031\n","Cost after iteration# 500: 0.027002\n","Cost after iteration# 600: 0.021543\n","Cost after iteration# 700: 0.017896\n","Cost after iteration# 800: 0.015293\n","Cost after iteration# 900: 0.013344\n","Cost after iteration# 1000: 0.011831\n","Cost after iteration# 1100: 0.010623\n","Cost after iteration# 1200: 0.009637\n","Cost after iteration# 1300: 0.008817\n","Cost after iteration# 1400: 0.008124\n","Cost after iteration# 1500: 0.007532\n","Cost after iteration# 1600: 0.007019\n","Cost after iteration# 1700: 0.006572\n","Cost after iteration# 1800: 0.006177\n","Cost after iteration# 1900: 0.005827\n","Cost after iteration# 2000: 0.005515\n","Cost after iteration# 2100: 0.005233\n","Cost after iteration# 2200: 0.004979\n","Cost after iteration# 2300: 0.004749\n","Cost after iteration# 2400: 0.004538\n","Cost after iteration# 2500: 0.004346\n","Cost after iteration# 2600: 0.004169\n","Cost after iteration# 2700: 0.004006\n","Cost after iteration# 2800: 0.003855\n","Cost after iteration# 2900: 0.003715\n","Cost after iteration# 3000: 0.003584\n","Cost after iteration# 3100: 0.003463\n","Cost after iteration# 3200: 0.003349\n","Cost after iteration# 3300: 0.003243\n","Cost after iteration# 3400: 0.003143\n","Cost after iteration# 3500: 0.003049\n","Cost after iteration# 3600: 0.002960\n","Cost after iteration# 3700: 0.002877\n","Cost after iteration# 3800: 0.002798\n","Cost after iteration# 3900: 0.002723\n","Cost after iteration# 4000: 0.002652\n","Cost after iteration# 4100: 0.002585\n","Cost after iteration# 4200: 0.002521\n","Cost after iteration# 4300: 0.002460\n","Cost after iteration# 4400: 0.002402\n","Cost after iteration# 4500: 0.002347\n","Cost after iteration# 4600: 0.002294\n","Cost after iteration# 4700: 0.002243\n","Cost after iteration# 4800: 0.002195\n","Cost after iteration# 4900: 0.002148\n","Cost after iteration# 5000: 0.002104\n","Cost after iteration# 5100: 0.002061\n","Cost after iteration# 5200: 0.002020\n","Cost after iteration# 5300: 0.001981\n","Cost after iteration# 5400: 0.001943\n","Cost after iteration# 5500: 0.001906\n","Cost after iteration# 5600: 0.001871\n","Cost after iteration# 5700: 0.001837\n","Cost after iteration# 5800: 0.001805\n","Cost after iteration# 5900: 0.001773\n","Cost after iteration# 6000: 0.001743\n","Cost after iteration# 6100: 0.001713\n","Cost after iteration# 6200: 0.001685\n","Cost after iteration# 6300: 0.001657\n","Cost after iteration# 6400: 0.001631\n","Cost after iteration# 6500: 0.001605\n","Cost after iteration# 6600: 0.001580\n","Cost after iteration# 6700: 0.001556\n","Cost after iteration# 6800: 0.001532\n","Cost after iteration# 6900: 0.001509\n","Cost after iteration# 7000: 0.001487\n","Cost after iteration# 7100: 0.001466\n","Cost after iteration# 7200: 0.001445\n","Cost after iteration# 7300: 0.001425\n","Cost after iteration# 7400: 0.001405\n","Cost after iteration# 7500: 0.001386\n","Cost after iteration# 7600: 0.001367\n","Cost after iteration# 7700: 0.001349\n","Cost after iteration# 7800: 0.001331\n","Cost after iteration# 7900: 0.001314\n","Cost after iteration# 8000: 0.001297\n","Cost after iteration# 8100: 0.001281\n","Cost after iteration# 8200: 0.001265\n","Cost after iteration# 8300: 0.001249\n","Cost after iteration# 8400: 0.001234\n","Cost after iteration# 8500: 0.001219\n","Cost after iteration# 8600: 0.001204\n","Cost after iteration# 8700: 0.001190\n","Cost after iteration# 8800: 0.001177\n","Cost after iteration# 8900: 0.001163\n","Cost after iteration# 9000: 0.001150\n","Cost after iteration# 9100: 0.001137\n","Cost after iteration# 9200: 0.001124\n","Cost after iteration# 9300: 0.001112\n","Cost after iteration# 9400: 0.001100\n","Cost after iteration# 9500: 0.001088\n","Cost after iteration# 9600: 0.001076\n","Cost after iteration# 9700: 0.001065\n","Cost after iteration# 9800: 0.001054\n","Cost after iteration# 9900: 0.001043\n","Cost after iteration# 10000: 0.001033\n","Neural Network prediction for example (1, 1) is 0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F2RI4KxEdVOF","colab_type":"text"},"source":["Shamefully ripped from https://towardsdatascience.com/how-to-build-a-simple-neural-network-from-scratch-with-python-9f011896d2f3 \n"]}]}